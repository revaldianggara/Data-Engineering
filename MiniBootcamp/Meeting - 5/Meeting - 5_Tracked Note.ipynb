{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMr4EBIAkEBXTXeuU/vL79P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-iZ5WCNhvkLi"},"outputs":[],"source":["import os\n","import json\n","os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-1.8\"\n","os.environ['PYSPARK_PYTHON'] = 'python'\n"]},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession, SQLContext\n"],"metadata":{"id":"DfCj68Jdwbe5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conf = SparkConf() \\\n","    .setAppName(\"Spark Sample - ETL\") \\\n","    .setMaster(\"local\") \\\n","    .set(\"spark.driver.extraClassPath\",\"C:/pyspark/*\")\n","\n","sc = SparkContext.getOrCreate(conf=conf)\n","spark = SparkSession(sc)"],"metadata":{"id":"YTP-zjpCwdxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initialize Data Extraction"],"metadata":{"id":"Hjz-1IG_wqGn"}},{"cell_type":"code","source":["df=spark.read.options(delimiter=\",\", header=True).csv(\"./sales_generated.csv\")"],"metadata":{"id":"272mQDoXwi6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.createOrReplaceTempView(\"national_sales\")"],"metadata":{"id":"MVuWAYR6wmvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Extracting individual date \"\"\"\n","\n","filtered_date = spark.sql(\"SELECT DISTINCT DATE(date) FROM national_sales ORDER BY 1 ASC\")\n","filtered_date.show()"],"metadata":{"id":"ob9Lpyxdw0gN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_json_date = filtered_date.toJSON().collect()"],"metadata":{"id":"qfwKnsyrw5JW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Iterate Transformation by each date and Load the transformed data into destionation \"\"\"\n","\n","# DATABASE TARGET CONFIG\n","dest_tbl = 'public.\"etl_sales_revenue_daily\"'\n","database = \"postgres\"\n","password = \"password\"\n","user = \"postgres\"\n","\n","\n","for data in evaluate_json_date[:5]:\n","    current_date = json.loads(data)['date']\n","    result_by_date = spark.sql(f\"\"\"\n","    SELECT\n","        DATE(date) as date,\n","        name,\n","        SUM(number_of_sales) as number_of_sales,\n","        SUM(number_of_sales * pricing_unit) as revenue\n","    FROM national_sales\n","    WHERE date RLIKE '{current_date}'\n","    GROUP BY DATE(date), name\n","    \"\"\")\n","    result_by_date.write.mode(\"append\") \\\n","    .format(\"jdbc\") \\\n","    .option(\"url\", f\"jdbc:postgresql://localhost:5432/{database}\") \\\n","    .option(\"dbtable\", dest_tbl) \\\n","    .option(\"user\", user) \\\n","    .option(\"password\", password) \\\n","    .option(\"driver\",  \"org.postgresql.Driver\") \\\n","    .save()"],"metadata":{"id":"cl7daSBLxClS"},"execution_count":null,"outputs":[]}]}